{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlD96dTNnE-6"
   },
   "source": [
    "# Assignment 1: Build a parser and tokenizer for Wikipedia content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD1VrZ0Meu0r"
   },
   "source": [
    "## General guidelines\n",
    "\n",
    "This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±\n",
    "\n",
    "**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Staff will allow your notebook to **execute from start to finish for no more than 90 seconds**, then manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. \n",
    "\n",
    "**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle. Before submitting, **make sure the notebook executes from start to finish in less than 90 seconds**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VccPFI-Sp57X"
   },
   "source": [
    "# Tasks\n",
    "\n",
    "In this assignment, we are going to read, parse, and tokenize a small number of Wikipedia articles from a single part file, and learn how to collect and merge page view information with articles. By the end of this assignment, you will be able to:\n",
    "\n",
    "1. (25 Points) Parse and clean Wikipedia page entries from one part file of the wiki dump. From each page, extract the title, body, and a list of (anchor text, page title) pairs for links to other English Wikipedia pages. \n",
    "2. (70 Points) Tokenize the text of Wikipedia articles using a regex tokenizer. \n",
    "3. (5 Points) Collect page view information from Wikipedia and merge that with articles' data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1666612540460,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "nAdNnF04t3Hd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bz2\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from xml.etree import ElementTree\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import gdown\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuDnZdXSsDJj"
   },
   "source": [
    "# 1. Parsing a dump file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4_pHk6vyc1G"
   },
   "source": [
    "First, let's download the file and examine it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 2077,
     "status": "ok",
     "timestamp": 1666612542535,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "Lx1Y81O5uX_o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f199b97535575015f87f958b7385abfe",
     "grade": false,
     "grade_id": "data-download",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "38bb3e25-d26a-46de-c0f5-af8fe7781df7"
   },
   "outputs": [],
   "source": [
    "## Download one wikipedia file\n",
    "path_url = 'https://drive.google.com/file/d/1c2ggRHG0WqLmJpIE-HfQgJi73kww3Zyc/view?usp=sharing'\n",
    "wiki_file = 'enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2'\n",
    "id = '1c2ggRHG0WqLmJpIE-HfQgJi73kww3Zyc'\n",
    "gdown.download(id=id, output=wiki_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1666612542536,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "HdlUgKJTujCu",
    "outputId": "555d142c-47fe-47ac-819b-a649ae772b44"
   },
   "outputs": [],
   "source": [
    "# Make sure you downloaded the file\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666612542536,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "ii20yBfBxIF-"
   },
   "outputs": [],
   "source": [
    "# Uncomment this to view the first 59 lines of the (uncompressed) file\n",
    "#!bzcat $wiki_file | head -n 59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABRNY_pByr8t"
   },
   "source": [
    "## Parsing from XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si3EIy8ADs2Z"
   },
   "source": [
    "The following code reads the wiki dump file, parses its XML, and iterate over pages. First, it filters out pages that are redirects to other pages, talk pages, and any other pages that are not articles pages. Then, it extracts the article id and article markup text.\n",
    "\n",
    "**YOUR TASK (5 Points)**: Implement the extraction of article title from the XML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666612542537,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "661_cqDiKdHy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18263a8a30a0c43531ec90c46b754e2b",
     "grade": false,
     "grade_id": "page_iter",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def page_iter(wiki_file):\n",
    "  \"\"\" Reads a wiki dump file and create a generator that yields pages. \n",
    "  Parameters:\n",
    "  -----------\n",
    "  wiki_file: str\n",
    "    A path to wiki dump file.\n",
    "  Returns:\n",
    "  --------\n",
    "  tuple\n",
    "    containing three elements: article id, title, and body. \n",
    "  \"\"\"\n",
    "  # open compressed bz2 dump file\n",
    "  with bz2.open(wiki_file, 'rt', encoding='utf-8', errors='ignore') as f_in:\n",
    "    # Create iterator for xml that yields output when tag closes\n",
    "    elems = (elem for _, elem in ElementTree.iterparse(f_in, events=(\"end\",)))\n",
    "    # Consume the first element and extract the xml namespace from it. \n",
    "    # Although the raw xml has the  short tag names without namespace, i.e. it \n",
    "    # has <page> tags and not <http://wwww.mediawiki.org/xml/export...:page> \n",
    "    # tags, the parser reads it *with* the namespace. Therefore, it needs the \n",
    "    # namespace when looking for child elements in the find function as below.\n",
    "    elem = next(elems)\n",
    "    m = re.match(\"^{(http://www\\.mediawiki\\.org/xml/export-.*?)}\", elem.tag)\n",
    "    if m is None:\n",
    "        raise ValueError(\"Malformed MediaWiki dump\")\n",
    "    ns = {\"ns\": m.group(1)}\n",
    "    page_tag = ElementTree.QName(ns['ns'], 'page').text\n",
    "    # iterate over elements\n",
    "    for elem in elems:\n",
    "      if elem.tag == page_tag:\n",
    "        # Filter out redirect and non-article pages\n",
    "        if elem.find('./ns:redirect', ns) is not None or \\\n",
    "           elem.find('./ns:ns', ns).text != '0':\n",
    "          elem.clear()\n",
    "          continue\n",
    "        # Extract the article wiki id\n",
    "        wiki_id = elem.find('./ns:id', ns).text\n",
    "        # Extract the article title into a variables called title\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # extract body\n",
    "        body = elem.find('./ns:revision/ns:text', ns).text\n",
    "\n",
    "        yield wiki_id, title, body\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666612542537,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "ZXuZ93MVXTpz",
    "outputId": "7c2447e8-3678-42ef-eaea-8ce09ef6b13b"
   },
   "outputs": [],
   "source": [
    "# Print the first page\n",
    "p1 = next(page_iter(wiki_file))\n",
    "print(f\"{p1[0]} {p1[1]}\\n\\n{' '*80}\\n{p1[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666612542537,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "ozXrWoJDysV5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "426cfe410e1f0a2b6e07be30cf819b0e",
     "grade": true,
     "grade_id": "extract_title",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the title of the first article\n",
    "assert p1[1] == 'Langnes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uaubgcMYClb"
   },
   "source": [
    "## Parsing articles (MediaWiki)\n",
    "\n",
    "Wikipedia articles are written in a special markdown format called [MediaWiki](https://en.wikipedia.org/wiki/MediaWiki). The format is described [here](https://www.mediawiki.org/wiki/Help:Formatting) and you will need to read a little bit about it in order to complete this assignment successfully. A key property of the MediaWiki markdown is that it's recursive -- markdown can be (and often does) nest inside other markdown elements. For example, a wiki link can contain another wiki link:\n",
    "```\n",
    "[[File:image1.jpg|[[Wikipedia]]]]\n",
    "```\n",
    "\n",
    "Fortunately, there are implementations of parsers for MediaWiki, and we are going to use one called `mwparserfromhell`. Let's import/install it first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4877,
     "status": "ok",
     "timestamp": 1666612547408,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "1o0r_AHP2GOJ",
    "outputId": "c4e77656-1eef-4b21-9a3d-f6716b23b549"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import mwparserfromhell as mwp\n",
    "except ImportError:\n",
    "    !pip install -I mwparserfromhell==0.6.0\n",
    "finally:\n",
    "    import mwparserfromhell as mwp\n",
    "# modify the parser behavior a bit, no need to understand this code.\n",
    "mwp.definitions.INVISIBLE_TAGS.append('ref')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmUhEFFf8Ww8"
   },
   "source": [
    "Let's parse some MediaWiki and look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666612547408,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "p3P9QXKG84QB",
    "outputId": "51e5869d-b182-4373-84e6-d4eff7e07507"
   },
   "outputs": [],
   "source": [
    "wikicode = mwp.parse('Lorem ipsum {{foo|bar|{{baz}}|spam=eggs}}')\n",
    "print(wikicode.get_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1Ds3wZ2-eM-"
   },
   "source": [
    "Naturally, the result of parsing the MediaWiki format results is **a tree**. In the above case, the markdown contains a [template](https://www.mediawiki.org/wiki/Help:Templates) named foo, which takes two positional parameters, another template called baz, and a named parameter called spam. Templates are both cool (in providing structured information) and wild (in the sense that their structure keeps changing). \n",
    "\n",
    "A particular type of template called **Infobox** is the backbone of some of the largest knowledge bases on the planet(!), powering DBPedia and many popular IR applications such as Apple's Siri, Google Search, and more. To get a sense of the richness of infoboxes, let's look at one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1666612547881,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "IiaDfnCxEVw2",
    "outputId": "be093153-0642-4213-df41-424e4c25b54c"
   },
   "outputs": [],
   "source": [
    "iter = page_iter(wiki_file)\n",
    "next(iter)\n",
    "p2 = next(iter)\n",
    "wikicode = mwp.parse(p2[2], skip_style_tags=True)\n",
    "for template in wikicode.ifilter_templates():\n",
    "  if str(template.name).strip().startswith('Infobox'):\n",
    "    print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu6etF_YiSwo"
   },
   "source": [
    "Look at all this beautifully-structured data about a random village in Norway!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFWKVm3XPLAJ"
   },
   "source": [
    "## Parsing outgoing article links (wiki links)\n",
    "\n",
    "Putting aside templates and infoboxes, one of the key elements we care about in search engines is links. The text of the link, also known as anchor text, is a very useful description of the page that is being linked. Analyzing the link structure between pages using algorithms like PageRank, which we will cover later in class, is one of the key factors for improving search results. \n",
    "\n",
    "In this assignment, we're going to focus on links from one wikipedia article to another wikipedia article.\n",
    "\n",
    "**YOUR TASK (20 POINTS)**: Complete the impementation of `get_wikilinks` and `filter_article_links` to return a list of (link, anchor text) for all outgoing links from an article to other wikipedia articles that it points to. See MediaWiki's format for [internal links](https://www.mediawiki.org/wiki/Help:Links#Internal_links). Please remove from the link any reference to a section/anchor in the target page. You are free to implement your filter through a regex or some other means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666612547881,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "ScR7lH6TYf9k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8299df874b6c7128131c500a17afc7c5",
     "grade": false,
     "grade_id": "links",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def filter_article_links(title):\n",
    "  \"\"\" Return false for wikilink titles (str) pointing to non-articles such as images, files, media and more (as described in the documentation).\n",
    "      Otherwise, returns true. \n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "def get_wikilinks(wikicode):\n",
    "  \"\"\" Traverses the parse tree for internal links and filter out non-article \n",
    "  links.\n",
    "  Parameters:\n",
    "  -----------\n",
    "  wikicode: mwp.wikicode.Wikicode\n",
    "    Parse tree of some WikiMedia markdown.\n",
    "  Returns:\n",
    "  --------\n",
    "  list of (link: str, anchor_text: str) pair\n",
    "    A list of outgoing links from the markdown to wikipedia articles.\n",
    "  \"\"\"\n",
    "  links = []\n",
    "  for wl in wikicode.ifilter_wikilinks():\n",
    "    # skip links that don't pass our filter\n",
    "    title = str(wl.title)\n",
    "    if not filter_article_links(title):\n",
    "      continue\n",
    "    # if text is None use title, otherwise strip markdown from the anchor text.\n",
    "    text = wl.text\n",
    "    if text is None:\n",
    "      text = title\n",
    "    else:\n",
    "      text = text.strip_code()\n",
    "    # remove any lingering section/anchor reference in the link\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    links.append((title, text))\n",
    "  return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666612547881,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "PCiW-Hq0rJet",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "276c9a282515f0e424ae468b6f37546d",
     "grade": true,
     "grade_id": "links-basic",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Basic checks that we can extract links correctly\n",
    "get_wl = lambda text: get_wikilinks(mwp.parse(text))\n",
    "assert get_wl(\"[[Wikipedia]]\")[0] == ('Wikipedia', 'Wikipedia')\n",
    "assert get_wl(\"[[Wikipedia|some text]]\")[0] == ('Wikipedia', 'some text')\n",
    "assert len(get_wl(\"[[File:example.jpg|frame|caption]]\")) == 0\n",
    "assert get_wl('[[Wikipedia#Preview|preview]]')[0] == ('Wikipedia', 'preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666612547882,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "C80Zs6L724OG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4ff4653b5984f1993ac64be339a15a0",
     "grade": true,
     "grade_id": "links-adv",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# More advanced tests\n",
    "assert len(get_wl(p2[2])) < len(mwp.parse(p2[2]).filter_wikilinks())\n",
    "pages = list(islice(page_iter(wiki_file), None, 25))\n",
    "p4, p14, p24 = pages[4], pages[14], pages[24]\n",
    "assert len(get_wl(p4[2])) == 32\n",
    "assert len(get_wl(p14[2])) == 891\n",
    "assert len(get_wl(p24[2])) == 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHZo7qtJDCI_"
   },
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl2o0OtM3sMn"
   },
   "source": [
    "Before tokenizing Wikipedia articles' text we need to remove any remaining MediaWiki markdown from the text. Luckily, our parser knows how to strip all markdown as demonstrated by the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1666612548300,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "58Mm27-Xmjvg",
    "outputId": "1e62356f-db46-4a1d-ec57-5e64eb80a612"
   },
   "outputs": [],
   "source": [
    "def remove_markdown(text):\n",
    "  return mwp.parse(text).strip_code()\n",
    "print(remove_markdown(\"\"\"\n",
    "== Section 2 ==\n",
    "[[File:image1.jpg| '''''beautiful''''' <b>image</b> of [[Wikipedia]]]]\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW_dZl9pG5Mz"
   },
   "source": [
    "Great! now we can focus on tokenzing the clean text. Here's the clean text of one article after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548300,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "zZa97L8KsrMo",
    "outputId": "4c40de07-4c13-4621-9b6f-f62bd0371a27"
   },
   "outputs": [],
   "source": [
    "print(remove_markdown(p4[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHZpZM6qT3MK"
   },
   "source": [
    "**YOUR TASK (70 POINTS)**: Complete the implementation of the functions in the next cell that return regular expressions (as strings) to capture dates, time, etc. in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548301,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "PoQ6TG0buRAL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64d35f0cee87a117caeb4f1516c59a62",
     "grade": false,
     "grade_id": "tok-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548301,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "tvNGAvTccGHw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0a5f6e870cb1ac99d637404738f67a0",
     "grade": false,
     "grade_id": "cell-218dd071f62181a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "RE_TOKENIZE = re.compile(rf\"\"\"\n",
    "(\n",
    "    # parsing html tags\n",
    "     (?P<HTMLTAG>{get_html_pattern()})                                  \n",
    "    # dates\n",
    "    |(?P<DATE>{get_date_pattern()})\n",
    "    # time\n",
    "    |(?P<TIME>{get_time_pattern()})\n",
    "    # Percents\n",
    "    |(?P<PERCENT>{get_percent_pattern()})\n",
    "    # Numbers\n",
    "    |(?P<NUMBER>{get_number_pattern()})\n",
    "    # Words\n",
    "    |(?P<WORD>{get_word_pattern()})\n",
    "    # space\n",
    "    |(?P<SPACE>[\\s\\t\\n]+) \n",
    "    # everything else\n",
    "    |(?P<OTHER>.))\"\"\",  re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "  return [(v, k) for match in RE_TOKENIZE.finditer(text)\n",
    "                 for k, v in match.groupdict().items() \n",
    "                 if v is not None and k != 'SPACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548301,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "IjSOkJ9rRhhf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55957def7ed9332ea774d8ebcb8d0915",
     "grade": true,
     "grade_id": "html-basic",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# html basic tests (7 points)\n",
    "def tok(text):\n",
    "  return tokenize(remove_markdown(text))\n",
    "tokens = tok(r'<nowiki><b>hello</b></nowiki>')\n",
    "assert ('<b>', 'HTMLTAG') in tokens\n",
    "assert ('</b>', 'HTMLTAG') in tokens\n",
    "tokens = tok(r'<nowiki><b style=\"color:red\">hello</b></nowiki>')\n",
    "assert ('<b style=\"color:red\">', 'HTMLTAG') in tokens\n",
    "tokens = tok(r'<nowiki><br /></nowiki>')\n",
    "assert ('<br />', 'HTMLTAG') in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548302,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "GT8hBUEJWuGV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fd0bcdae93fa1e299649566d2851e1b",
     "grade": true,
     "grade_id": "html-adv",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# html advanced tests (6 points)\n",
    "tokens = tok(r'<nowiki><b><i>hello</i></b></nowiki>')\n",
    "assert 4 == sum([1 for _, t in tokens if t == 'HTMLTAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548302,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "qab7jLsOX_jH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84440df7d383a67d60384c79fb9bf1b7",
     "grade": true,
     "grade_id": "dates-basic",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dates basic tests (7 points)\n",
    "tokens = tok(r'dates in the format of January 29, 1984, Nov 3, 2020, or 3 Nov 2020.')\n",
    "assert ('January 29, 1984', 'DATE') in tokens\n",
    "assert ('Nov 3, 2020', 'DATE') in tokens\n",
    "assert ('3 Nov 2020', 'DATE') in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548303,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "1ngQ08sVaL-1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08a2960dc053c12a3c5e5f79b23682f2",
     "grade": true,
     "grade_id": "dates-adv",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dates advanced tests (6 points)\n",
    "tokens = tok(r'Sep 29, 1984, Apr 33, 2020, or 30 feb 2020.')\n",
    "assert ('Sep 29, 1984', 'DATE') in tokens\n",
    "assert ('Apr 33, 2020', 'DATE') not in tokens\n",
    "assert ('30 feb 2020', 'DATE') not in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548303,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "iDE8L6WxD8IJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22dc3e966cdd3ad42ea22edd9cf6c6ec",
     "grade": true,
     "grade_id": "time-basic",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# time basic tests (7 points)\n",
    "tokens = tok(r'12.12PM 1202a.m. 6:12:12')\n",
    "assert ('12.12PM', 'TIME') in tokens\n",
    "assert ('1202a.m.', 'TIME') in tokens\n",
    "assert ('6:12:12', 'TIME') in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548303,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "V7YzBe-8INrQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4cdfc9a1967c9afaeebcc6a25102330",
     "grade": true,
     "grade_id": "time-adv",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# time advanced tests (6 points)\n",
    "tokens = tok(r'36.12PM 1272a.m. 1202a.m 12:12:12am 56:12:12 6:72:12')\n",
    "assert 0 == sum([1 for _, t in tokens if t == 'TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548304,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "BuWJFc9Wg3bf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3758c0adc0fa462bc77a34bba2947ab9",
     "grade": true,
     "grade_id": "num-basic",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# number basic tests (7 points)\n",
    "tokens = tok(r\"\"\"12 +12 -12.0 -12,345.5466 +12,345,678,678 0.154\"\"\")\n",
    "assert ('12', 'NUMBER') in tokens\n",
    "assert ('+12', 'NUMBER') in tokens\n",
    "assert ('-12.0', 'NUMBER') in tokens\n",
    "assert ('-12,345.5466', 'NUMBER') in tokens\n",
    "assert ('+12,345,678,678', 'NUMBER') in tokens\n",
    "assert ('0.154', 'NUMBER') in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548304,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "eG33yKNhDPjY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "773499a6e2cbd3bfe06517024f6ecd5e",
     "grade": true,
     "grade_id": "num-adv",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# number advanced tests (6 points)\n",
    "assert ('500', 'NUMBER') in tok('the pound (500 in value)...')\n",
    "assert ('500', 'NUMBER') in tok('the price is 500.')\n",
    "assert ('500', 'NUMBER') in tok('the price is 500, but it is negotiable.')\n",
    "assert ('500', 'NUMBER') in tok('the price is 500: no less!')\n",
    "assert ('500', 'NUMBER') not in tok('the price rose 500%')\n",
    "tokens = tok(r\"\"\"12.A W12 +-12 -.12.0 -12,34.5466 +12,345,6+78,678 0.15,4\"\"\")\n",
    "assert 0 == sum([1 for _, t in tokens if t == 'NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1666612548304,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "B_m0bKwoPuZU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff76ea4b448fed3e9ad28c119f898ad0",
     "grade": true,
     "grade_id": "words",
     "locked": true,
     "points": 13,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# word tests (13 points)\n",
    "tokens = tok(r\"\"\"Hello Bob! It's Mary, your mother-in-law, \n",
    "  the mistake is your parents'! --Mom\"\"\")\n",
    "assert ('Hello', 'WORD') in tokens\n",
    "assert ('Bob', 'WORD') in tokens\n",
    "assert (\"It's\", 'WORD') in tokens\n",
    "assert ('Mary', 'WORD') in tokens\n",
    "assert ('your', 'WORD') in tokens\n",
    "assert ('mother-in-law', 'WORD') in tokens\n",
    "assert (\"parents'\", 'WORD') in tokens\n",
    "assert (\"Mom\", 'WORD') not in tokens\n",
    "assert (\"-Mom\", 'WORD') not in tokens\n",
    "assert (\"--Mom\", 'WORD') not in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548305,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "J8YG_fyHgKjw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "260e3cd54ed67e65d63fbde9343a8306",
     "grade": true,
     "grade_id": "tok-comp",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# comprehensiveness test (5 points)\n",
    "_, t = zip(*tok(pages[9][2]))\n",
    "assert 5 == len(set(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2hBIAQRlCZB"
   },
   "source": [
    "# 3. Collect and merge page views\n",
    "\n",
    "Data about page views on Wikipedia is available at https://dumps.wikimedia.org and there is documentation about the [definition of a page view](https://meta.wikimedia.org/wiki/Research:Page_view) and the [format of lines](https://dumps.wikimedia.org/other/pagecounts-ez/) in the file. In the class project, you will need to use page view data that we'll provide for ALL of English Wikipedia from the month of August 2021, which is more than 10.7 million viewed articles. The commented out code below shows how we generate that data, no need to run it yourself, this is just for your information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548305,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "9Hv947ULyiT5"
   },
   "outputs": [],
   "source": [
    "# # Paths\n",
    "# # Using user page views (as opposed to spiders and automated traffic) for the \n",
    "# # month of August 2021\n",
    "# pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "# p = Path(pv_path) \n",
    "# pv_name = p.name\n",
    "# pv_temp = f'{p.stem}-4dedup.txt'\n",
    "# pv_clean = f'{p.stem}.pkl'\n",
    "# # Download the file (2.3GB) \n",
    "# !wget -N $pv_path\n",
    "# # Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
    "# # total number of page views (5). Then, remove lines with article id or page \n",
    "# # view values that are not a sequence of digits.\n",
    "# !bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
    "# # Create a Counter (dictionary) that sums up the pages views for the same \n",
    "# # article, resulting in a mapping from article id to total page views.\n",
    "# wid2pv = Counter()\n",
    "# with open(pv_temp, 'rt') as f:\n",
    "#   for line in f:\n",
    "#     parts = line.split(' ')\n",
    "#     wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# # write out the counter as binary file (pickle it)\n",
    "# with open(pv_clean, 'wb') as f:\n",
    "#   pickle.dump(wid2pv, f)\n",
    "# # read in the counter\n",
    "# # with open(pv_clean, 'rb') as f:\n",
    "# #   wid2pv = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baQSJ9-CbKFR"
   },
   "source": [
    "In order to keep things simple, in this assignment we provide you with a small sample of articles and their page view counts.\n",
    "\n",
    "**YOUR TASK (5 POINTS)**: Complete the implementation of `most_viewed` for ranking articles from the most viewed to the least viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548305,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "fvW_GMjYcI4u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cd167f1f4610328560383f81a8137c2",
     "grade": false,
     "grade_id": "page-view-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A counter mapping article id to number of page views\n",
    "wid2pv = Counter({\n",
    "    '17324616': 10, '17324662': 4, '17324672': 16, '17324677': 612, \n",
    "    '17324689': 66, '17324702': 274, '17324704': 49, '17324943': 76, \n",
    "    '17324721': 35, '17324736': 2801, '17324747': 641, '17324758': 33,\n",
    "    '17324768': 26, '17324783': 28, '17324788': 575, '17324790': 43, \n",
    "    '17324802': 29, '17324816': 159, '17324818': 57, '17324823': 60,\n",
    "    '17324834': 19, '17324835': 7, '17324893': 116, '17324908': 2038,\n",
    "    '15580374': 181126232, '1610886': 4657885, '30635': 8143874, \n",
    "    '3390': 4525604, '49632909': 5027640, '51150040': 3284643, \n",
    "    '60827': 4323859, '623737': 3427102, '65984422': 3733064, '737': 6039676\n",
    "})\n",
    "\n",
    "def most_viewed(pages):\n",
    "  \"\"\"Rank pages from most viewed to least viewed using the above `wid2pv` \n",
    "     counter.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    pages: An iterable list of pages as returned from `page_iter` where each \n",
    "           item is an article with (id, title, body)\n",
    "  Returns:\n",
    "  --------\n",
    "  A list of tuples\n",
    "    Sorted list of articles from most viewed to least viewed article with \n",
    "    article title and page views. For example:\n",
    "    [('Langnes, Troms': 16), ('Langenes': 10), ('Langenes, Finnmark': 4), ...]\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666612548306,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "R7HwO9KugJOm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c210d29457d4684081c22d0423fe2c2f",
     "grade": true,
     "grade_id": "page-views-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pages_ranked = most_viewed(pages)\n",
    "_, views = zip(*pages_ranked)\n",
    "assert 25 == len(pages_ranked)\n",
    "assert ('No W', 274) in pages_ranked\n",
    "assert 7774 == sum(views)\n",
    "assert 2226 == views[0] - views[4]\n",
    "assert 10 == round(sum(views[0:5]) / sum(views[5:10]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
